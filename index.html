<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0JKBJ3WRJZ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-0JKBJ3WRJZ');
    </script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3&display=swap" rel="stylesheet">
    <meta charset="UTF-8">
    <title>DeepSound-V1</title>

    <link rel="icon" type="image/png" href="images/icon.png">

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- CSS only -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

    <link rel="stylesheet" href="style.css">
</head>
<body>

    <body>
        <br><br><br><br>
        <div class="container">
            <div class="row text-center" style="font-size:38px">
                <div class="col strong">
                  DeepSound-V1: Start to Think Step-by-Step in the Audio Generation from Videos
                </div>
            </div>
			
            <br>
            <br>
    
            <div class="h-100 row text-center justify-content-md-center" style="font-size:20px;">
                <div class="col-sm-2">
                    <a href="https://arxiv.org">[Paper]</a>
                </div>
                <div class="col-sm-2">
                    <a href="https://github.com/lym0302/DeepSound-V1">[Code]</a>
                </div>
                <div class="col-sm-3">
                    <a href="https://huggingface.co/spaces/lym0302/DeepSound-V1">[Huggingface Demo]</a>
                </div>
				<div class="col-sm-2">
                    <a href="https://huggingface.co/lym0302/VideoLLaMA2.1-7B-AV-CoT">[Models]</a>
                </div>
            </div>
			
			<br>
            <br>
			
			<hr>
			
			<br>
            <br>
			
			<div class="row" style="font-size:32px">
                <div class="col strong">
                    Abstract
                </div>
            </div>
			
			<br>
            <br>
			
            Currently, high-quality, synchronized audio is synthesized from video and optional text inputs using 
            various multi-modal joint learning frameworks. However, the precise alignment between the visual and 
            generated audio domains remains far from satisfactory. One key factor is the lack of sufficient temporal 
            and semantic alignment annotations in open-source video-audio and text-audio benchmarks. Therefore, we 
            propose a framework for audio generation from videos, leveraging the internal chain-of-thought (CoT) of 
            a multi-modal large language model (MLLM) to enable step-by-step reasoning without requiring additional 
            annotations. Additionally, a corresponding multi-modal reasoning dataset is constructed to facilitate the 
            learning of initial reasoning in audio generation. In the experiments, we demonstrate the effectiveness 
            of the proposed framework in reducing misalignment (voice-over) in generated audio and achieving competitive 
            performance compared to various state-of-the-art models. The evaluation results show that the proposed 
            method outperforms state-of-the-art approaches across multiple metrics. Specifically, the $FD_{PaSST}$ 
            indicator is reduced by up to 10.07\%, the $FD_{PANNs}$ indicator by up to 11.62\%, and the $FD_{VGG}$ 
            indicator by up to 38.61\%. Furthermore, the IS indicator improves by up to 4.95\%, the IB-score indicator 
            increases by up to 6.39\%, and the DeSync indicator is reduced by up to 0.89\%.
			
			<br>
			<br>
			
			<hr>
			
			<br>
            <br>
			
            <div class="row" style="font-size:32px">
                <div class="col strong">
                    Demos
                </div>
            </div>
			
            <br>
			<br>
			
			<!-- <div class="row" style="font-size:32px">
                <div class="col strong">
                    Video-to-Speech
                </div>
            </div>
			
			<br>
			<br> -->
			
			<div style="padding: 0 0; text-align: center; display: flex; justify-content: space-around;">
				<p style="text-align: center;">Ground Truth</p>
				<p style="text-align: center;">Generated Audios</p>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000001.gt.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000001.gen.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000003.gt.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000003.gen.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000024.gt.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000024.gen.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000025.gt.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000025.gen.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000111.gt.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000111.gen.mp4">
				</video>
			</div>

			<br>
			<br>
			
			
			
            <br>
            <br>
			
            <hr>
			
			<br>
            <br>
			
			<div class="row" style="font-size:32px">
                <div class="col strong">
                    Method
                </div>
            </div>
			
			<br>
            <br>
            
			<img src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/images/Fig_2_2.png" alt="overview" style="width:100%;">
			
			<br>
			
			Figure 1: The overview of DeepAudio framework. The proposed DeepAudio framework unifies video-to-audio (V2A) and video-to-speech (V2S) generation in a multi-stage, end-to-end paradigm. The top section illustrates two independent generation paths: (1) The V2A module, which synthesizes ambient audio from video input using a CLIP-based multi-modal feature encoding and a noised latent representation, and (2) The TTS module, which generates speech conditioned on text and noised latent features. Both modules rely on codec decoders to reconstruct high-fidelity outputs. The bottom section presents the MoF module, an integrated multi-modal system that takes text, video, and instructions as inputs. A Gating Network adaptively fuses outputs from the V2A module and the TTS module, ensuring synchronized and context-aware audio-visual generation.
			
			<br>
            <br>
			
			<img src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/images/specs_3.jpg" alt="spectrograms" style="width:100%;">
			
			<br>
			
			Figure 2: Mel-spectrograms of ground truth and synthesized audio samples from different methods under V2C-Animation Dub 2.0 setting.
			
			<br>
            <br>
			
			<hr>
			
			<br>
            <br>
			
			<div class="row" style="font-size:32px">
                <div class="col strong">
                    Results
                </div>
            </div>
			
			<br>
            <br>
			
			<table border="1" cellpadding="5" cellspacing="0" style="width: 100%; text-align: center;">
			  <thead>
				<tr>
				  <th rowspan="2">V2S Task</th>
				  <th></th>
				  <th colspan="5">V2C-Animation Dub 1.0</th>
				  <th colspan="5">V2C-Animation Dub 2.0</th>
				  <th colspan="2">V2C-Animation Dub 3.0</th>
				</tr>
				<tr>
				  <th>Params</th>
				  <th>WER(%)↓</th>
				  <th>SPK-SIM(%)↑</th>
				  <th>EMO-SIM(%)↑</th>
				  <th>MCD↓</th>
				  <th>MCD_SL↓</th>
				  <th>WER(%)↓</th>
				  <th>SPK-SIM(%)↑</th>
				  <th>EMO-SIM(%)↑</th>
				  <th>MCD↓</th>
				  <th>MCD_SL↓</th>
				  <th>WER(%)↓</th>
				  <th>SPK-SIM(%)↑</th>
				</tr>
			  </thead>
			  <tbody>
				<tr>
				  <td>GT</td>
				  <td></td>
				  <td>16.10</td>
				  <td>100</td>
				  <td>100</td>
				  <td>0</td>
				  <td>0</td>
				  <td>16.10</td>
				  <td>100</td>
				  <td>100</td>
				  <td>0</td>
				  <td>0</td>
				  <td>10.19</td>
				  <td>100</td>
				</tr>
				<tr>
				  <td>HPMDubbing</td>
				  <td>69M</td>
				  <td>151.02</td>
				  <td>73.64</td>
				  <td>39.85</td>
				  <td>8.59</td>
				  <td>8.32</td>
				  <td>150.83</td>
				  <td>73.01</td>
				  <td>34.69</td>
				  <td><strong>9.11</strong></td>
				  <td>12.15</td>
				  <td>126.85</td>
				  <td>68.14</td>
				</tr>
				<tr>
				  <td>Speaker2Dub</td>
				  <td>160M</td>
				  <td>31.23</td>
				  <td>82.15</td>
				  <td>65.92</td>
				  <td>10.68</td>
				  <td>11.21</td>
				  <td>31.28</td>
				  <td>79.53</td>
				  <td>59.71</td>
				  <td>11.16</td>
				  <td>11.70</td>
				  <td>16.57</td>
				  <td>76.10</td>
				</tr>
				<tr>
				  <td>StyleDubber</td>
				  <td>163M</td>
				  <td>27.36</td>
				  <td>82.48</td>
				  <td>66.24</td>
				  <td>10.06</td>
				  <td>10.52</td>
				  <td>26.48</td>
				  <td>79.81</td>
				  <td>59.08</td>
				  <td>10.56</td>
				  <td>11.05</td>
				  <td>19.07</td>
				  <td>78.30</td>
				</tr>
				<tr>
				  <td>Ours-MM-S16K</td>
				  <td>493M</td>
				  <td>7.16</td>
				  <td>89.20</td>
				  <td>75.27</td>
				  <td>8.02</td>
				  <td>8.11</td>
				  <td>10.29</td>
				  <td>83.83</td>
				  <td>65.70</td>
				  <td>9.33</td>
				  <td>9.43</td>
				  <td>3.17</td>
				  <td>89.30</td>
				</tr>
				<tr>
				  <td>Ours-MM-S44K</td>
				  <td>493M</td>
				  <td><strong>6.90</strong></td>
				  <td><strong>89.22</strong></td>
				  <td><strong>75.56</strong></td>
				  <td><strong>7.98</strong></td>
				  <td><strong>8.07</strong></td>
				  <td>10.46</td>
				  <td>83.83</td>
				  <td>65.65</td>
				  <td>9.30</td>
				  <td><strong>9.40</strong></td>
				  <td>3.15</td>
				  <td>89.38</td>
				</tr>
				<tr>
				  <td>Ours-MM-M44K</td>
				  <td>957M</td>
				  <td>7.24</td>
				  <td>89.12</td>
				  <td>75.37</td>
				  <td>8.01</td>
				  <td>8.10</td>
				  <td><strong>10.24</strong></td>
				  <td>83.78</td>
				  <td><strong>65.83</strong></td>
				  <td>9.31</td>
				  <td>9.41</td>
				  <td>3.15</td>
				  <td><strong>89.39</strong></td>
				</tr>
				<tr>
				  <td>Ours-MM-L44K</td>
				  <td>1.37B</td>
				  <td>7.18</td>
				  <td>89.19</td>
				  <td>75.41</td>
				  <td><strong>7.98</strong></td>
				  <td><strong>8.07</strong></td>
				  <td>10.40</td>
				  <td><strong>83.87</strong></td>
				  <td>65.70</td>
				  <td>9.31</td>
				  <td>9.42</td>
				  <td><strong>3.03</strong></td>
				  <td>89.22</td>
				</tr>
				<tr>
				  <td>Ours-YS-24K</td>
				  <td>1.05B</td>
				  <td>7.33</td>
				  <td>89.14</td>
				  <td>75.49</td>
				  <td>8.01</td>
				  <td>8.10</td>
				  <td>10.58</td>
				  <td>83.84</td>
				  <td><strong>65.83</strong></td>
				  <td>9.32</td>
				  <td>9.43</td>
				  <td>3.10</td>
				  <td>89.21</td>
				</tr>
			  </tbody>
			  <thead>
				<tr>
				  <th rowspan="2">V2A Task</th>
				  <th></th>
				  <th colspan="6">VGGSound-Test</th>
				</tr>
				<tr>
				  <th>Params</th>
				  <th>FAD↓</th>
				  <th>FD↓</th>
				  <th>KL↓</th>
				  <th>IS↑</th>
				  <th>CLIP↑</th>
				  <th>AV↑</th>
				</tr>
			  </thead>
			  <tbody>
				<tr>
				  <td>Diff-Foley</td>
				  <td>859M</td>
				  <td>6.05</td>
				  <td>23.38</td>
				  <td>3.18</td>
				  <td>10.95</td>
				  <td>9.40</td>
				  <td>0.21</td>
				</tr>
				<tr>
				  <td>FoleyCrafter w/o text</td>
				  <td>1.22B</td>
				  <td>2.38</td>
				  <td>26.70</td>
				  <td>2.53</td>
				  <td>9.66</td>
				  <td>15.57</td>
				  <td><strong>0.25</strong></td>
				</tr>
				<tr>
				  <td>FoleyCrafter w. text</td>
				  <td>1.22B</td>
				  <td>2.59</td>
				  <td>20.88</td>
				  <td>2.28</td>
				  <td>13.60</td>
				  <td>14.80</td>
				  <td>0.24</td>
				</tr>
				<tr>
				  <td>V2A-Mapper</td>
				  <td>229M</td>
				  <td>0.82</td>
				  <td>13.47</td>
				  <td>2.67</td>
				  <td>10.53</td>
				  <td>15.33</td>
				  <td>0.14</td>
				</tr>
				<tr>
				  <td>Frieren</td>
				  <td>159M</td>
				  <td>1.36</td>
				  <td>12.48</td>
				  <td>2.75</td>
				  <td>12.34</td>
				  <td>11.57</td>
				  <td>0.21</td>
				</tr>
				<tr>
				  <td>Ours-MM-S16K</td>
				  <td>157M</td>
				  <td>0.79</td>
				  <td>5.22</td>
				  <td><strong>1.65</strong></td>
				  <td>14.44</td>
				  <td>15.22</td>
				  <td>0.21</td>
				</tr>
				<tr>
				  <td>Ours-MM-S44K</td>
				  <td>157M</td>
				  <td>1.66</td>
				  <td>5.55</td>
				  <td>1.67</td>
				  <td><strong>18.02</strong></td>
				  <td>15.38</td>
				  <td>0.21</td>
				</tr>
				<tr>
				  <td>Ours-MM-M44K</td>
				  <td>621M</td>
				  <td>1.13</td>
				  <td>4.74</td>
				  <td>1.66</td>
				  <td>17.41</td>
				  <td>15.89</td>
				  <td>0.22</td>
				</tr>
				<tr>
				  <td>Ours-MM-L44K</td>
				  <td>1.03B</td>
				  <td>0.97</td>
				  <td><strong>4.72</strong></td>
				  <td><strong>1.65</strong></td>
				  <td>17.40</td>
				  <td>16.12</td>
				  <td>0.22</td>
				</tr>
				<tr>
				  <td>Ours-YS-24K</td>
				  <td>718M</td>
				  <td><strong>0.74</strong></td>
				  <td>5.69</td>
				  <td>1.69</td>
				  <td>14.63</td>
				  <td><strong>17.70</strong></td>
				  <td>0.24</td>
				</tr>
			  </tbody>
			  <thead>
				<tr>
				  <th rowspan="2">TTS Task</th>
				  <th></th>
				  <th colspan="2">LibriSpeech-PC test-clean</th>
				</tr>
				<tr>
				  <th>Params</th>
				  <th>SIM-O↑</th>
				  <th>WER(%)↓</th>
				</tr>
			  </thead>
			  <tbody>
				<tr>
				  <td>GT</td>
				  <td></td>
				  <td>0.665</td>
				  <td>2.281</td>
				</tr>
				<tr>
				  <td>F5-TTS</td>
				  <td>336M</td>
				  <td>0.648</td>
				  <td>2.508</td>
				</tr>
				<tr>
				  <td>Ours-MM-S16K</td>
				  <td>336M</td>
				  <td><strong>0.650</strong></td>
				  <td>2.267</td>
				</tr>
				<tr>
				  <td>Ours-MM-S44K</td>
				  <td>336M</td>
				  <td>0.648</td>
				  <td>2.499</td>
				</tr>
				<tr>
				  <td>Ours-MM-M44K</td>
				  <td>336M</td>
				  <td>0.648</td>
				  <td>2.510</td>
				</tr>
				<tr>
				  <td>Ours-MM-L44K</td>
				  <td>336M</td>
				  <td>0.647</td>
				  <td><strong>2.175</strong></td>
				</tr>
				<tr>
				  <td>Ours-YS-24K</td>
				  <td>336M</td>
				  <td>0.647</td>
				  <td>2.521</td>
				</tr>
			  </tbody>
			</table>
			
			<br>
			
			Table 1: Comparison of different methods on V2S, V2A and TTS tasks. Ours-MM-* represents the V2A module is a reproduced MMAudio series model with specific parameter size and audio sampling rate, while Our-YS-24K represents the V2A module is a reproduced YingSound model. The Params of the Ours-* model refers to the number of actually activated parameters, which depends on the specific task.
    
            <br><br>
            <br><br>
    
        </div>

</body>
</html>