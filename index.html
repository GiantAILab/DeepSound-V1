<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG" />
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta
      property="og:description"
      content="SOCIAL MEDIA DESCRIPTION TAG TAG"
    />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG" />
    <meta
      name="twitter:description"
      content="TWITTER BANNER DESCRIPTION META TAG"
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta
      name="twitter:image"
      content="static/images/your_twitter_banner_image.png"
    />
    <meta name="twitter:card" content="summary_large_image" />
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>DeepDubber-V1</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico" />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
  </head>
  <body>
    <section class="hero is-light">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                DeepDubber-V1: Towards High Quality and Dialogue, Narration,
                Monologue Adaptive Movie Dubbing Via Multi-Modal
                Chain-of-Thoughts Reasoning Guidance
              </h1>
              <!-- <div class="is-size-5 publication-authors"> -->
              <!-- Paper authors -->
              <!-- <span class="author-block"> -->
              <!-- <a href="FIRST AUTHOR PERSONAL LINK" target="_blank" -->
              <!-- >First Author</a -->
              <!-- ><sup>*</sup>,</span -->
              <!-- > -->
              <!-- <span class="author-block"> -->
              <!-- <a href="SECOND AUTHOR PERSONAL LINK" target="_blank" -->
              <!-- >Second Author</a -->
              <!-- ><sup>*</sup>,</span -->
              <!-- > -->
              <!-- <span class="author-block"> -->
              <!-- <a href="THIRD AUTHOR PERSONAL LINK" target="_blank" -->
              <!-- >Third Author</a -->
              <!-- > -->
              <!-- </span> -->
              <!-- </div> -->

              <!-- <div class="is-size-5 publication-authors">
                <span class="author-block"
                  >Institution Name<br />Conferance name and year</span
                >
                <span class="eql-cntrb"
                  ><small
                    ><br /><sup>*</sup>Indicates Equal Contribution</small
                  ></span
                >
              </div> -->

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Supplementary PDF link -->
                  <!-- <span class="link-block">
                    <a
                      href="static/pdfs/supplementary_material.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href="https://github.com/woka-0a/DeepDubber-V1"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/<ARXIV PAPER ID>"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Paper abstract -->
    <section class="section hero">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Current movie dubbing technology can generate the desired voice
                from a given speech prompt, ensuring good synchronization
                between speech and visuals while accurately conveying the
                intended emotions. However, in movie dubbing, key aspects such
                as adapting to different dubbing styles, handling dialogue,
                narration, and monologue effectively, and understanding subtle
                details like the age and gender of speakers, have not been well
                studied. To address this challenge, we propose a framework of
                multimodal large language model. First, it utilizes multimodal
                Chain-of-Thought (CoT) reasoning methods on visual inputs to
                understand dubbing styles and fine-grained attributes. Second,
                it generates high-quality dubbing through large speech
                generation models, guided by multimodal conditions.
                Additionally, we have developed a movie dubbing dataset with CoT
                annotations. The evaluation results demonstrate a performance
                improvement over state-of-the-art methods across multiple
                datasets. In particular, for the evaluation metrics, the SPK-SIM
                and EMO-SIM increases from 82.48% to 89.74%, 66.24% to 78.88%
                for dubbing setting 2.0 on V2C-Animation dataset, LSE-D and
                MCD-SL decreases from 14.79 to 14.63, 5.24 to 4.74 for dubbing
                setting 2.0 on Grid dataset, SPK-SIM increases from 64.03 to
                83.42 and WER decreases from 52.69% to 23.20% for initial
                reasoning setting on proposed CoT-Movie-Dubbing dataset in the
                comparison with the state-of-the art models.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!-- Image carousel -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Model & Dataset Details</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <img
                src="static/images/deepdubber01.png"
                alt="MY ALT TEXT"
                class="center-image"
              />
              <h2 class="subtitle has-text-centered">
                DeepDubber pipeline with multi-stage, multi-modal training.
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img
                src="static/images/dataset_cot.png"
                alt="MY ALT TEXT"
                class="center-image"
              />
              <h2 class="subtitle has-text-centered">
                The reasoning stages of movie scene type CoT annotations.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->

    <!-- Teaser Image-->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Main Results</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img
                src="static/images/r1o1.png"
                alt="MY ALT TEXT"
                class="center-image"
              />
              <h2 class="subtitle has-text-centered">
                Objective evaluation of the initial reasoning setting. For
                speech generation setting, we use the target speaker's speech as
                voice prompt if the predict scene type is correct and use random
                speaker's speech as voice prompt if the predict scene type is
                not correct.
              </h2>
            </div>
            <div class="item">
              <img
                src="static/images/r2o1.png"
                alt="MY ALT TEXT"
                class="center-image"
              />
              <h2 class="subtitle has-text-centered">
                Ablation of objective evaluation under the initial reasoning
                setting on the proposed dataset. For the speech generation
                setting, we use the target speaker's speech as a voice prompt if
                the predicted scene type is correct, and a random speaker's
                speech if the predicted scene type is incorrect. F&O Reward
                refers to format reward and outcome reward.
              </h2>
            </div>
            <div class="item">
              <img
                src="static/images/r3o1.png"
                alt="MY ALT TEXT"
                class="center-image"
              />
              <h2 class="subtitle has-text-centered">
                Objective evaluation of the Reasoning Stage based on scores in
                dialogue (A), monologue (B), and narration (C) under two
                different levels.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End teaser Image -->

    <!-- Video carousel -->
    <!-- Your video file here -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Demo Videos</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-video1">
              <video
                poster=""
                id="video1"
                controls
                muted
                loop
                class="center-video"
              >
                <source
                  src="https://github.com/user-attachments/assets/5d78401e-efc6-4034-b66b-047aad129338"
                  type="video/mp4"
                />
              </video>
            </div>
            <div class="item item-video2">
              <video
                poster=""
                id="video2"
                controls
                muted
                loop
                class="center-video"
              >
                <source
                  src="https://github.com/user-attachments/assets/58ab9155-c088-44f8-be5e-1967e01a94bc"
                  type="video/mp4"
                />
              </video>
            </div>
            <div class="item item-video3">
              <video
                poster=""
                id="video3"
                controls
                muted
                loop
                class="center-video"
              >
                \
                <source
                  src="https://github.com/user-attachments/assets/360893bd-6e06-470f-b6f2-0ef723cb8dc7"
                  type="video/mp4"
                />
              </video>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- Your video file here -->
    <!-- End video carousel -->

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>BibTex Code Here</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the <a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                > project page.<br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>
