<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0JKBJ3WRJZ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-0JKBJ3WRJZ');
    </script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3&display=swap" rel="stylesheet">
    <meta charset="UTF-8">
    <title>DeepSound-V1</title>

    <link rel="icon" type="image/png" href="images/icon.png">

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- CSS only -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

    <link rel="stylesheet" href="style.css">
</head>
<body>

        <br><br><br><br>
        <div class="container">
            <div class="row text-center" style="font-size:38px">
                <div class="col strong">
                  DeepSound-V1: Start to Think Step-by-Step in the Audio Generation from Videos
                </div>
            </div>
			
            <br>
            <br>
    
            <div class="h-100 row text-center justify-content-md-center" style="font-size:20px;">
                <div class="col-sm-2">
                    <a href="https://arxiv.org">[Paper]</a>
                </div>
                <div class="col-sm-2">
                    <a href="https://github.com/lym0302/DeepSound-V1">[Code]</a>
                </div>
                <div class="col-sm-3">
                    <a href="https://huggingface.co/spaces/lym0302/DeepSound-V1">[Huggingface Demo]</a>
                </div>
				<div class="col-sm-2">
                    <a href="https://huggingface.co/lym0302/VideoLLaMA2.1-7B-AV-CoT">[Models]</a>
                </div>
            </div>
			
			<br>
            <br>
			
			<hr>
			
			<br>
            <br>
			
			<div class="row" style="font-size:32px">
                <div class="col strong">
                    Abstract
                </div>
            </div>
			
			<br>
            <br>
			
            Currently, high-quality, synchronized audio is synthesized from video and optional text inputs using 
            various multi-modal joint learning frameworks. However, the precise alignment between the visual and 
            generated audio domains remains far from satisfactory. One key factor is the lack of sufficient temporal 
            and semantic alignment annotations in open-source video-audio and text-audio benchmarks. Therefore, we 
            propose a framework for audio generation from videos, leveraging the internal chain-of-thought (CoT) of 
            a multi-modal large language model (MLLM) to enable step-by-step reasoning without requiring additional 
            annotations. Additionally, a corresponding multi-modal reasoning dataset is constructed to facilitate the 
            learning of initial reasoning in audio generation. In the experiments, we demonstrate the effectiveness 
            of the proposed framework in reducing misalignment (voice-over) in generated audio and achieving competitive 
            performance compared to various state-of-the-art models. The evaluation results show that the proposed 
            method outperforms state-of-the-art approaches across multiple metrics. Specifically, the FD<sub>PaSST</sub> 
            indicator is reduced by up to 10.07%, the FD<sub>PANNs</sub> indicator by up to 11.62%, and the FD<sub>VGG</sub>
            indicator by up to 38.61%. Furthermore, the IS indicator improves by up to 4.95%, the IB-score indicator 
            increases by up to 6.39%, and the DeSync indicator is reduced by up to 0.89%.
			
			<br>
			<br>
			
			<hr>
			
			<br>
            <br>
			
            <div class="row" style="font-size:32px">
                <div class="col strong">
                    Demos
                </div>
            </div>
			
            <br>
			<br>
			
			<!-- <div class="row" style="font-size:32px">
                <div class="col strong">
                    Video-to-Speech
                </div>
            </div> -->
			
			<br>
			<br>
			
			<div style="padding: 0 0; text-align: center; display: flex; justify-content: space-around;">
				<p style="text-align: center;">Generate Directly</p>
				<p style="text-align: center;">Generate Step-by-Step</p>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_direct/iEhCQX3a44k_000313.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_stepbs/iEhCQX3a44k_000313.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_direct/JdzRzkTzmMw_000098.mp4">
          
				</video>
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_stepbs/JdzRzkTzmMw_000098.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_direct/jE8MWTGaM7U_000030.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_stepbs/jE8MWTGaM7U_000030.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_direct/nC3S8LBLL9I_000564.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_stepbs/nC3S8LBLL9I_000564.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_direct/QnR9JhVGgu8_000348.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_stepbs/QnR9JhVGgu8_000348.mp4">
				</video>
			</div>
      <div class="row">
				<video style="width: 49%;" controls>
					<source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_directR63TQ4aEGYA_000410.mp4">
				</video>
				<video style="width: 49%;" controls>
					<!-- <source src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/videos_stepbs/R63TQ4aEGYA_000410.mp4"> -->
          <source src="assets/videos_stepbs/R63TQ4aEGYA_000410.mp4", type="video/mp4">
				</video>
			</div>

			<br>
			<br>
			
			
            <hr>
			
			<br>
            <br>
			
			<div class="row" style="font-size:32px">
                <div class="col strong">
                    Method
                </div>
            </div>
			
			<br>
            <br>
            
			<!-- <img src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/images/figure1_overview.png" alt="overview_deepsound" style="width:100%;"> -->
			
      <img src="https://github.com/user-attachments/assets/7070156c-816a-4300-98e0-58b32488cd37" alt="overview_deepsound" style="width:100%;">

			<br>
			
			Figure 1: Overview of DeepSound. The model employs a step-by-step reasoning process to generate audio from video. In the first step, it
      generates a coarse audio from the input video. The second step identifies voice-over components by analyzing both the coarse audio and
      the video. The third step removes the detected voice-over elements from the audio. Finally, the model determines whether the resulting
      audio is silent or not.
			
			<br>
            <br>
            
			
			<img src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/images/figure2_mllm.jpg" alt="overview_mllm" style="width:100%;">
			
			<br>
			
			Figure 2: Overview of Dual Multi-Modal Reasoning Learning. CoTstructure represents the internal reasoning steps within the overall
      audio generation process. CoTdetail refers to the step-by-step procedure for identifying voice-over components from the coarse audio and
      video.
			
			<br>
            <br>
			
			<hr>
			
			<br>
            <br>
			
			<div class="row" style="font-size:32px">
                <div class="col strong">
                    Main Results
                </div>
            </div>
			
            <br>
            <br>
            
			<img src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/images/table1.png" alt="main_res" style="width:100%;">
			
			<br>
			
			Table 1: Video-to-audio results on the VGGSound test set. The bold text highlights the superior performance of our proposed method
      compared to previous methods, while the green text in brackets represents the improvement rate of each index.

      <br>
            <br>
           
			
			<img src="https://github.com/lym0302/DeepSound-V1/blob/pages/assets/images/table2.jpg" alt="ablation" style="width:100%;">
			
			<br>
			
			Table 2: Ablation result on MMAudio-L-44k. The improvement between baseline and ours is represented as green color, demonstrating
      effectiveness of the learned CoT reasoning in enhancing the final audio quality, the improvement between Ours-s3 and Ours-s4 is represented as blue color.
			
			<br><br>
            <br><br>



      </div>

</body>
</html>